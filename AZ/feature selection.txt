모델을 구성하기 위한 피처를 선택하는 과정이다. 장점은 다음과 같다.
​- 사용자가 더 해석하기 쉽게 모델을 단순화
- 훈련 시간의 축소
- 차원의 저주 방지
- 오버피팅을 줄여, 좀 더 일반화
Feature Selection을 한다는 것은 하위 셋을 만들기 위한 과정이기에 시간과 자원이 충분하다면 2^N-1가지 방법을 모두 테스트하여 구하고자하는 점수가 높은 subset을 사용하면 됩니다. 이 방법은 현실적으로 무리기 때문에 평가 메트릭에 따라 적합한 방법을 사용하는 것이 좋습니다. 크게 3가지 분류를 하자면 다음과 같습니다.
- Wrapper method
- Filter method
- Embedded method
Wrapper method 는 예측 모델을 사용하여 피처 subset을 계속 테스트합니다. 이 경우 기존 데이터에서 테스트를 진행할 hold-out set을 따로두어야 합니다. 이렇게 subset을 체크하면 어떤 feature가 필요한지 알 수 있습니다. 하지만 이는 Computing Power가 비약적으로 큰 NP 문제이고, 그렇기에 random hill-climbing과 같은 휴리스틱 방법론을 사용합니다.
Filter Method는 통계적 측정 방법을 사용하여 피처들의 상관관계를 알아냅니다. 하지만 피처간의 상관계수가 반드시 모델에 적합한 피처라고는 할 수 없고, 세트의 조정이 정확하지 않습니다. 대신 계산속도가 빠르고 피처간 상관관계를 알아내는데 적합하기 때문에 Wrapper method를 사용하기 전에 전처리하는데 사용할 수 있습니다.
Embedded method 는 모델의 정확도에 기여하는 피처를 학습합니다. 좀 더 적은 계수를 가지는 회귀식을 찾는 방향으로 제약조건을 주어 이를 제어합니다.
훈련할 데이터에서 feature를 고른다면, 훈련 데이터에 과적합될 수 있습니다. 그러므로 훈련 데이터, 테스트 데이터를 제외한 데이터에서 선택하는 것이 중요합니다. 모든 데이터에서 feature selection을 진행하면, 교차 검증에서 똑같이 선택된 feature를 사용하게 되므로 결과가 편향될 수 있습니다.